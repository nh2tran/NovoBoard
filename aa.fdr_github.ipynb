{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ef5061",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17005/3466405393.py:22: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  display.set_matplotlib_formats('svg')\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "display.set_matplotlib_formats('svg')\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8552cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f45d6",
   "metadata": {
    "code_folding": [
     0,
     21,
     62,
     68,
     98,
     376,
     390
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate fragment ion, amino acid, and peptide accuracies\n",
    "print(stop)\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "import os.path\n",
    "import config\n",
    "\n",
    "\n",
    "col_precursor_mz = \"m/z\"\n",
    "col_precursor_charge = \"z\"\n",
    "col_rt_mean = \"RT\"\n",
    "col_raw_sequence = \"Peptide\"\n",
    "col_source_file = \"Source File\"\n",
    "col_scan_list = \"Scan\"\n",
    "\n",
    "\n",
    "def parse_raw_sequence(raw_sequence: str):\n",
    "    raw_sequence_len = len(raw_sequence)\n",
    "    peptide = []\n",
    "    index = 0\n",
    "    while index < raw_sequence_len:\n",
    "        if raw_sequence[index] == \"(\":\n",
    "            if peptide[-1] == \"C\" and raw_sequence[index:index + 8] == \"(+57.02)\":\n",
    "                peptide[-1] = \"C(Carbamidomethylation)\"\n",
    "                index += 8\n",
    "            elif peptide[-1] == 'M' and raw_sequence[index:index + 8] == \"(+15.99)\":\n",
    "                peptide[-1] = 'M(Oxidation)'\n",
    "                index += 8\n",
    "            elif peptide[-1] == 'N' and raw_sequence[index:index + 7] == \"(+0.98)\":\n",
    "                peptide[-1] = 'N(Deamidation)'\n",
    "                index += 7\n",
    "            elif peptide[-1] == 'Q' and raw_sequence[index:index + 7] == \"(+0.98)\":\n",
    "                peptide[-1] = 'Q(Deamidation)'\n",
    "                index += 7\n",
    "            elif peptide[-1] == 'S' and raw_sequence[index:index + 8] == \"(+79.97)\":\n",
    "                peptide[-1] = \"S(Phosphorylation)\"\n",
    "                index += 8\n",
    "            elif peptide[-1] == 'T' and raw_sequence[index:index + 8] == \"(+79.97)\":\n",
    "                peptide[-1] = \"T(Phosphorylation)\"\n",
    "                index += 8\n",
    "            elif peptide[-1] == 'Y' and raw_sequence[index:index + 8] == \"(+79.97)\":\n",
    "                peptide[-1] = \"Y(Phosphorylation)\"\n",
    "                index += 8\n",
    "            else:  # unknown modification\n",
    "                # logger.warning(f\"unknown modification in seq {raw_sequence}\")\n",
    "                return False, peptide\n",
    "        else:\n",
    "            peptide.append(raw_sequence[index])\n",
    "            index += 1\n",
    "\n",
    "    for aa in peptide:\n",
    "        if aa not in config.vocab:\n",
    "            # logger.warning(f\"unknown modification in seq {raw_sequence}\")\n",
    "            return False, peptide\n",
    "    return True, peptide\n",
    "\n",
    "\n",
    "class WorkerTest(object):\n",
    "  \"\"\"TODO(nh2tran): docstring.\n",
    "     The WorkerTest should be stand-alone and separated from other workers.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, target_file, predicted_file, spectrum_file):\n",
    "    \"\"\"TODO(nh2tran): docstring.\"\"\"\n",
    "\n",
    "    print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    print(\"WorkerTest.__init__()\")\n",
    "\n",
    "    # we currently use deepnovo_config to store both const & settings\n",
    "    # the settings should be shown in __init__() to keep track carefully\n",
    "    self.MZ_MAX = config.MZ_MAX\n",
    "\n",
    "    self.target_file = target_file\n",
    "    self.predicted_file = predicted_file\n",
    "    self.spectrum_file = spectrum_file\n",
    "    self.accuracy_file = predicted_file + \".accuracy\"\n",
    "    self.denovo_only_file = predicted_file + \".denovo_only\"\n",
    "    self.scan2fea_file = predicted_file + \".scan2fea\"\n",
    "    self.multifea_file = predicted_file + \".multifea\"\n",
    "    print(\"target_file = {0:s}\".format(self.target_file))\n",
    "    print(\"predicted_file = {0:s}\".format(self.predicted_file))\n",
    "    print(\"spectrum_file = {0:s}\".format(self.spectrum_file))\n",
    "    print(\"accuracy_file = {0:s}\".format(self.accuracy_file))\n",
    "    print(\"denovo_only_file = {0:s}\".format(self.denovo_only_file))\n",
    "    print(\"scan2fea_file = {0:s}\".format(self.scan2fea_file))\n",
    "    print(\"multifea_file = {0:s}\".format(self.multifea_file))\n",
    "\n",
    "    self.target_dict = {}\n",
    "    self.predicted_list = []\n",
    "    self.spectrum_dict = {}\n",
    "\n",
    "\n",
    "  def test_accuracy(self, db_peptide_list=None):\n",
    "    \"\"\"TODO(nh2tran): docstring.\"\"\"\n",
    "\n",
    "    print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    print(\"WorkerTest.test_accuracy()\")\n",
    "\n",
    "    # write the accuracy of predicted peptides\n",
    "    accuracy_handle = open(self.accuracy_file, 'w')\n",
    "    header_list = [\"feature_id\",\n",
    "                   \"feature_area\",\n",
    "                   \"target_sequence\",\n",
    "                   \"predicted_sequence\",\n",
    "                   \"predicted_score\",\n",
    "                   \"predicted_aa_score\",\n",
    "                   \"recall_AA\",\n",
    "                   \"aa_match\",\n",
    "                   \"predicted_len\",\n",
    "                   \"target_len\",\n",
    "                   \"target_ion\",\n",
    "                   \"matched_ion\",\n",
    "                   \"unmatched_ion_list\",\n",
    "                   \"scan_list_middle\",\n",
    "                   \"scan_list_original\"]\n",
    "    header_row = \"\\t\".join(header_list)\n",
    "    print(header_row, file=accuracy_handle, end=\"\\n\")\n",
    "\n",
    "    # write denovo_only peptides\n",
    "    denovo_only_handle = open(self.denovo_only_file, 'w')\n",
    "    header_list = [\"feature_id\",\n",
    "                   \"feature_area\",\n",
    "                   \"predicted_sequence\",\n",
    "                   \"predicted_score\",\n",
    "                   \"predicted_score_max\",\n",
    "                   \"scan_list_middle\",\n",
    "                   \"scan_list_original\"]\n",
    "    header_row = \"\\t\".join(header_list)\n",
    "    print(header_row, file=denovo_only_handle, end=\"\\n\")\n",
    "\n",
    "    self._get_target()\n",
    "    target_count_total = len(self.target_dict)\n",
    "    target_len_total = sum([len(x) for x in self.target_dict.values()])\n",
    "\n",
    "    # this part is tricky!\n",
    "    # some target peptides are reported by PEAKS DB but not found in\n",
    "    #   db_peptide_list due to mistakes in cleavage rules.\n",
    "    # if db_peptide_list is given, we only consider those target peptides,\n",
    "    #   otherwise, use all target peptides\n",
    "    target_dict_db = {}\n",
    "    if db_peptide_list is not None:\n",
    "      for feature_id, target in self.target_dict.items():\n",
    "        target_simplied = target\n",
    "        # remove the extension 'mod' from variable modifications\n",
    "        target_simplied = ['M' if x=='M(Oxidation)' else x for x in target_simplied]\n",
    "        target_simplied = ['N' if x=='N(Deamidation)' else x for x in target_simplied]\n",
    "        target_simplied = ['Q' if x=='Q(Deamidation)' else x for x in target_simplied]\n",
    "        if target_simplied in db_peptide_list:\n",
    "          target_dict_db[feature_id] = target\n",
    "        else:\n",
    "          print(\"target not found: \", target_simplied)\n",
    "    else:\n",
    "      target_dict_db = self.target_dict\n",
    "    target_count_db = len(target_dict_db)\n",
    "    target_len_db = sum([len(x) for x in target_dict_db.values()])\n",
    "\n",
    "    # we also skip target peptides with precursor_mass > MZ_MAX\n",
    "    target_dict_db_mass = {}\n",
    "    for feature_id, peptide in target_dict_db.items():\n",
    "      if self._compute_peptide_mass(peptide) <= self.MZ_MAX:\n",
    "        target_dict_db_mass[feature_id] = peptide\n",
    "    target_count_db_mass = len(target_dict_db_mass)\n",
    "    target_len_db_mass = sum([len(x) for x in target_dict_db_mass.values()])\n",
    "\n",
    "    # read predicted peptides from deepnovo or peaks\n",
    "    self._get_predicted_peaks_11()\n",
    "\n",
    "    # note that the prediction has already skipped precursor_mass > MZ_MAX\n",
    "    # we also skip predicted peptides whose feature_id's are not in target_dict_db_mass\n",
    "    predicted_count_mass = len(self.predicted_list)\n",
    "    predicted_count_mass_db = 0\n",
    "    predicted_len_mass_db = 0\n",
    "    predicted_only = 0\n",
    "    # the recall is calculated on remaining peptides\n",
    "    recall_AA_total = 0.0\n",
    "    recall_peptide_total = 0.0\n",
    "    # fragment ion accuracy\n",
    "    target_ion_total = 0.0\n",
    "    predicted_ion_total = 0.0\n",
    "    matched_ion_total = 0.0\n",
    "    recall_all_peptide_ions_total = 0.0\n",
    "\n",
    "    # record scan with multiple features\n",
    "    scan_dict = {}\n",
    "\n",
    "    # read spectra to calculate fragment ion accuracy\n",
    "    self._get_spectra()\n",
    "    \n",
    "    id_set = set()\n",
    "    for index, predicted in enumerate(self.predicted_list):\n",
    "\n",
    "      feature_id = predicted[\"feature_id\"]\n",
    "      if feature_id in id_set:\n",
    "        continue\n",
    "      else:\n",
    "        id_set.add(feature_id)\n",
    "\n",
    "      feature_area = str(predicted[\"feature_area\"])\n",
    "      feature_scan_list_middle = predicted[\"scan_list_middle\"]\n",
    "      feature_scan_list_original = predicted[\"scan_list_original\"]\n",
    "      if feature_scan_list_original:\n",
    "        for scan in re.split(';|\\r|\\n', feature_scan_list_original):\n",
    "          if scan in scan_dict:\n",
    "            scan_dict[scan][\"feature_count\"] += 1\n",
    "            scan_dict[scan][\"feature_list\"].append(feature_id)\n",
    "          else:\n",
    "            scan_dict[scan] = {}\n",
    "            scan_dict[scan][\"feature_count\"] = 1\n",
    "            scan_dict[scan][\"feature_list\"] = [feature_id]\n",
    "\n",
    "      if feature_id in target_dict_db_mass:\n",
    "\n",
    "        predicted_count_mass_db += 1\n",
    "\n",
    "        target = target_dict_db_mass[feature_id]\n",
    "        target_len= len(target)\n",
    "\n",
    "        # if >= 1 denovo peptides reported, calculate the best accuracy\n",
    "        best_recall_AA = -1\n",
    "        best_aa_match = ''\n",
    "        best_predicted_sequence = predicted[\"sequence\"][0]\n",
    "        best_predicted_score = predicted[\"score\"][0]\n",
    "        best_predicted_aa_score = predicted[\"aa_score\"][0]\n",
    "        for predicted_sequence, predicted_score, predicted_aa_score in zip(predicted[\"sequence\"], predicted[\"score\"], predicted[\"aa_score\"]):\n",
    "          predicted_AA_id = [config.vocab[x] for x in predicted_sequence]\n",
    "          target_AA_id = [config.vocab[x] for x in target]\n",
    "          recall_AA, aa_match = self._match_AA_novor(target_AA_id, predicted_AA_id)\n",
    "          if (recall_AA > best_recall_AA\n",
    "              or (recall_AA == best_recall_AA and predicted_score > best_predicted_score)):\n",
    "            best_recall_AA = recall_AA\n",
    "            best_aa_match = aa_match\n",
    "            best_predicted_sequence = predicted_sequence[:]\n",
    "            best_predicted_score = predicted_score\n",
    "            best_predicted_aa_score = predicted_aa_score\n",
    "        recall_AA = best_recall_AA\n",
    "        aa_match = best_aa_match\n",
    "        predicted_sequence = best_predicted_sequence[:]\n",
    "        predicted_score = best_predicted_score\n",
    "        predicted_aa_score = best_predicted_aa_score\n",
    "\n",
    "        recall_AA_total += recall_AA\n",
    "        if recall_AA == target_len:\n",
    "          recall_peptide_total += 1\n",
    "        predicted_len= len(predicted_sequence)\n",
    "        predicted_len_mass_db += predicted_len\n",
    "\n",
    "        if feature_id in self.spectrum_dict:\n",
    "            target_ion, predicted_ion, matched_ion, unmatched_ion_list = self._match_ion(target, predicted_sequence, self.spectrum_dict[feature_id])\n",
    "        else:\n",
    "            target_ion, predicted_ion, matched_ion, unmatched_ion_list = 0, 0, 0, ''\n",
    "        target_ion_total += target_ion\n",
    "        predicted_ion_total += predicted_ion\n",
    "        matched_ion_total += matched_ion\n",
    "        recall_all_peptide_ions = matched_ion == target_ion\n",
    "        recall_all_peptide_ions_total += recall_all_peptide_ions\n",
    "\n",
    "        # convert to string format to print out\n",
    "        target_sequence = \",\".join(target)\n",
    "        predicted_sequence = \",\".join(predicted_sequence)\n",
    "        predicted_score = \"{0:.2f}\".format(predicted_score)\n",
    "        recall_AA = \"{0:d}\".format(recall_AA)\n",
    "        predicted_len = \"{0:d}\".format(predicted_len)\n",
    "        target_len = \"{0:d}\".format(target_len)\n",
    "        target_ion = \"{0:d}\".format(target_ion)\n",
    "        matched_ion = \"{0:d}\".format(matched_ion)\n",
    "        print_list = [feature_id,\n",
    "                      feature_area,\n",
    "                      target_sequence,\n",
    "                      predicted_sequence,\n",
    "                      predicted_score,\n",
    "                      predicted_aa_score,\n",
    "                      recall_AA,\n",
    "                      aa_match,\n",
    "                      predicted_len,\n",
    "                      target_len,\n",
    "                      target_ion,\n",
    "                      matched_ion,\n",
    "                      unmatched_ion_list,\n",
    "                      feature_scan_list_middle,\n",
    "                      feature_scan_list_original]\n",
    "        print_row = \"\\t\".join(print_list)\n",
    "        print(print_row, file=accuracy_handle, end=\"\\n\")\n",
    "      else:\n",
    "        predicted_only += 1\n",
    "        predicted_sequence = ';'.join([','.join(x) for x in predicted[\"sequence\"]])\n",
    "        predicted_score = ';'.join(['{0:.2f}'.format(x) for x in predicted[\"score\"]])\n",
    "        if predicted[\"score\"]:\n",
    "          predicted_score_max = '{0:.2f}'.format(np.max(predicted[\"score\"]))\n",
    "        else:\n",
    "          predicted_score_max = ''\n",
    "        print_list = [feature_id,\n",
    "                      feature_area,\n",
    "                      predicted_sequence,\n",
    "                      predicted_score,\n",
    "                      predicted_score_max,\n",
    "                      feature_scan_list_middle,\n",
    "                      feature_scan_list_original]\n",
    "        print_row = \"\\t\".join(print_list)\n",
    "        print(print_row, file=denovo_only_handle, end=\"\\n\")\n",
    "\n",
    "    accuracy_handle.close()\n",
    "    denovo_only_handle.close()\n",
    "\n",
    "    multifea_dict = {}\n",
    "    for scan_id, value in scan_dict.items():\n",
    "      feature_count = value[\"feature_count\"]\n",
    "      feature_list = value[\"feature_list\"]\n",
    "      if feature_count > 1:\n",
    "        for feature_id in feature_list:\n",
    "          if feature_id in multifea_dict:\n",
    "            multifea_dict[feature_id].append(scan_id + ':' + str(feature_count))\n",
    "          else:\n",
    "            multifea_dict[feature_id] = [scan_id + ':' + str(feature_count)]\n",
    "\n",
    "    with open(self.scan2fea_file, 'w') as handle:\n",
    "      header_list = [\"scan_id\",\n",
    "                     \"feature_count\",\n",
    "                     \"feature_list\"]\n",
    "      header_row = \"\\t\".join(header_list)\n",
    "      print(header_row, file=handle, end=\"\\n\")\n",
    "      for scan_id, value in scan_dict.items():\n",
    "        print_list = [scan_id,\n",
    "                      str(value[\"feature_count\"]),\n",
    "                      \";\".join(value[\"feature_list\"])]\n",
    "        print_row = \"\\t\".join(print_list)\n",
    "        print(print_row, file=handle, end=\"\\n\")\n",
    "\n",
    "    with open(self.multifea_file, 'w') as handle:\n",
    "      header_list = [\"feature_id\",\n",
    "                     \"scan_list\"]\n",
    "      header_row = \"\\t\".join(header_list)\n",
    "      print(header_row, file=handle, end=\"\\n\")\n",
    "      for feature_id, scan_list in multifea_dict.items():\n",
    "        print_list = [feature_id,\n",
    "                      \";\".join(scan_list)]\n",
    "        print_row = \"\\t\".join(print_list)\n",
    "        print(print_row, file=handle, end=\"\\n\")\n",
    "\n",
    "    print(\"target_count_total = {0:d}\".format(target_count_total))\n",
    "    print(\"target_len_total = {0:d}\".format(target_len_total))\n",
    "    print(\"target_count_db = {0:d}\".format(target_count_db))\n",
    "    print(\"target_len_db = {0:d}\".format(target_len_db))\n",
    "    print(\"target_count_db_mass: {0:d}\".format(target_count_db_mass))\n",
    "    print(\"target_len_db_mass: {0:d}\".format(target_len_db_mass))\n",
    "    print()\n",
    "\n",
    "    print(\"predicted_count_mass: {0:d}\".format(predicted_count_mass))\n",
    "    print(\"predicted_count_mass_db: {0:d}\".format(predicted_count_mass_db))\n",
    "    print(\"predicted_len_mass_db: {0:d}\".format(predicted_len_mass_db))\n",
    "    print(\"predicted_only: {0:d}\".format(predicted_only))\n",
    "    print()\n",
    "\n",
    "    print(\"recall_AA_total = {0:.4f}\".format(recall_AA_total / target_len_total))\n",
    "    print(\"recall_AA_db = {0:.4f}\".format(recall_AA_total / target_len_db))\n",
    "    print(\"recall_AA_db_mass = {0:.4f}\".format(recall_AA_total / target_len_db_mass))\n",
    "    print(\"recall_peptide_total = {0:.4f}\".format(recall_peptide_total / target_count_total))\n",
    "    print(\"recall_peptide_db = {0:.4f}\".format(recall_peptide_total / target_count_db))\n",
    "    print(\"recall_peptide_db_mass = {0:.4f}\".format(recall_peptide_total / target_count_db_mass))\n",
    "    print(\"precision_AA_mass_db  = {0:.4f}\".format(recall_AA_total / predicted_len_mass_db))\n",
    "    print(\"precision_peptide_mass_db  = {0:.4f}\".format(recall_peptide_total / predicted_count_mass_db))\n",
    "    print()\n",
    "\n",
    "    print(\"recall_ion = {0:.4f}\".format(matched_ion_total / target_ion_total))\n",
    "    print(\"precision_ion = {0:.4f}\".format(matched_ion_total / predicted_ion_total))\n",
    "    print(\"recall_all_peptide_ions = {0:.4f}\".format(recall_all_peptide_ions_total / target_count_db_mass))\n",
    "    print(\"target_ion_total =\", target_ion_total)\n",
    "    print(\"matched_ion_total =\", matched_ion_total)\n",
    "    print()\n",
    "\n",
    "  \n",
    "  def _compute_peptide_mass(self, peptide):\n",
    "    \"\"\"TODO(nh2tran): docstring.\n",
    "    \"\"\"\n",
    "\n",
    "    #~ print(\"\".join([\"=\"] * 80)) # section-separating line ===\n",
    "    #~ print(\"WorkerDB: _compute_peptide_mass()\")\n",
    "\n",
    "    peptide_mass = (config.mass_N_terminus\n",
    "                    + sum(config.mass_AA[aa] for aa in peptide)\n",
    "                    + config.mass_C_terminus)\n",
    "\n",
    "    return peptide_mass\n",
    "\n",
    "\n",
    "  def _get_predicted_peaks_11(self): # nh2tran\n",
    "    \"\"\"TODO(nh2tran): docstring.\"\"\"\n",
    "\n",
    "    print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    print(\"WorkerTest._get_predicted_peaks_11()\")\n",
    "\n",
    "    predicted_list = []\n",
    "    with open(self.predicted_file, 'r') as handle:\n",
    "        csv_reader = csv.DictReader(handle)\n",
    "        for row in csv_reader:\n",
    "            predicted = {}\n",
    "            # predicted[\"feature_id\"] = row[col_source_file] + \"||\" + row[col_scan_list]\n",
    "            predicted[\"feature_id\"] = row[col_source_file].split('.mgf')[0] + '.mgf' + \"||\" + row[col_scan_list]\n",
    "            # predicted[\"feature_id\"] = 'mutated_peptides1.mgf' + \"||\" + row['Scan']\n",
    "            raw_sequence = row[\"Peptide\"]\n",
    "            assert raw_sequence, \"Error: wrong format.\"\n",
    "            okay, predicted[\"sequence\"] = parse_raw_sequence(raw_sequence)\n",
    "            if not okay:\n",
    "                # skip unknown mod\n",
    "                continue\n",
    "            # skip peptides with precursor_mass > MZ_MAX\n",
    "            if self._compute_peptide_mass(predicted[\"sequence\"]) > self.MZ_MAX:\n",
    "                continue\n",
    "            predicted[\"feature_area\"] = 0\n",
    "            predicted[\"scan_list_middle\"] = \"\"\n",
    "            predicted[\"scan_list_original\"] = \"\"\n",
    "            predicted[\"sequence\"] = [predicted[\"sequence\"]]\n",
    "            predicted[\"score\"] = [float(row[col_score])]\n",
    "            predicted[\"aa_score\"] = [row[col_aa_score]]\n",
    "            predicted_list.append(predicted)\n",
    "\n",
    "    self.predicted_list = predicted_list\n",
    "\n",
    "  \n",
    "  def _get_target(self):\n",
    "    \"\"\"TODO(nh2tran): docstring.\"\"\"\n",
    "\n",
    "    print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    print(\"WorkerTest._get_target()\")\n",
    "\n",
    "    target_dict = {}\n",
    "    with open(self.target_file, 'r') as handle:\n",
    "      header_line = handle.readline()\n",
    "      header = [x.strip('\"') for x in header_line.strip().split(',')]\n",
    "      print(header)\n",
    "      raw_sequence_index = header.index(col_raw_sequence)\n",
    "      source_file_index = header.index(col_source_file)\n",
    "      scan_index = header.index(col_scan_list)\n",
    "\n",
    "      for line in handle:\n",
    "        line = [x.strip('\"') for x in re.split(',|\\r|\\n', line)]\n",
    "        feature_id = line[source_file_index] + \"||\" + line[scan_index]\n",
    "        raw_sequence = line[raw_sequence_index]\n",
    "        assert raw_sequence, \"Error: wrong target format.\"\n",
    "        okay, peptide = parse_raw_sequence(raw_sequence)\n",
    "        if not okay:\n",
    "          # skip unknown mod\n",
    "          continue\n",
    "        target_dict[feature_id] = peptide\n",
    "    self.target_dict = target_dict\n",
    "\n",
    "  \n",
    "  def _get_spectra(self):\n",
    "    \"\"\"TODO(nh2tran): docstring.\"\"\"\n",
    "\n",
    "    print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    print(\"WorkerTest._get_spectra()\")\n",
    "\n",
    "    with open(self.spectrum_file, 'r') as f_in:\n",
    "        while True:\n",
    "            line = f_in.readline()\n",
    "            if not line: # end of file\n",
    "                break\n",
    "            if line == '\\n': # empty line\n",
    "                continue\n",
    "            peak_list = []\n",
    "            while not \"END IONS\" in line:\n",
    "                # parse header lines\n",
    "                if 'BEGIN IONS' in line or '=' in line:\n",
    "                    if \"TITLE=\" in line:\n",
    "                        # source_file, refined_scan = re.split('[=\\r\\n]', line)[1].split('\\\\')[-1].rsplit('-', 1)\n",
    "                        source_file = re.split('[=\\r\\n]', line)[1].split('\\\\')[-1].split('.raw')[0] + '.mgf'\n",
    "                        # source_file = self.spectrum_file.split('/')[-1]#'mutated_peptides1.mgf'\n",
    "                    if line[:6] == \"SCANS=\":\n",
    "                        scan = re.split('[=\\r\\n]', line)[1]\n",
    "                    line = f_in.readline()\n",
    "                    continue\n",
    "                # parse ions\n",
    "                mz, intensity = re.split(' |\\t|\\r|\\n', line)[:2]\n",
    "                peak_list.append((float(mz), float(intensity)))\n",
    "                line = f_in.readline()\n",
    "            feature_id = source_file + '||' + scan\n",
    "            self.spectrum_dict[feature_id] = peak_list\n",
    "    print(\"len(self.spectrum_dict) =\", len(self.spectrum_dict))\n",
    "    print()\n",
    "\n",
    "  \n",
    "  def _match_AA_novor(self, target, predicted):\n",
    "    \"\"\"TODO(nh2tran): docstring.\"\"\"\n",
    "  \n",
    "    #~ print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    #~ print(\"WorkerTest._test_AA_match_novor()\")\n",
    "\n",
    "    num_match = 0\n",
    "    target_len = len(target)\n",
    "    predicted_len = len(predicted)\n",
    "    target_mass = [config.mass_ID[x] for x in target]\n",
    "    target_mass_cum = np.cumsum(target_mass)\n",
    "    predicted_mass = [config.mass_ID[x] for x in predicted]\n",
    "    predicted_mass_cum = np.cumsum(predicted_mass)\n",
    "  \n",
    "    i = 0\n",
    "    j = 0\n",
    "    aa_match = []\n",
    "    while i < target_len and j < predicted_len:\n",
    "      if abs(target_mass_cum[i] - predicted_mass_cum[j]) < 0.5:\n",
    "        if abs(target_mass[i] - predicted_mass[j]) < 0.1:\n",
    "        #~ if  decoder_input[index_aa] == output[index_aa]:\n",
    "          num_match += 1\n",
    "          aa_match.append('1')\n",
    "        else:\n",
    "          aa_match.append('0')\n",
    "        i += 1\n",
    "        j += 1\n",
    "      elif target_mass_cum[i] < predicted_mass_cum[j]:\n",
    "        i += 1\n",
    "      else:\n",
    "        j += 1\n",
    "        aa_match.append('0')\n",
    "    aa_match = ' '.join(aa_match)\n",
    "\n",
    "    return num_match, aa_match\n",
    "\n",
    "\n",
    "  def _peptide_to_ions(self, peptide):\n",
    "    \"\"\"TODO(nh2tran): docstring.\"\"\"\n",
    "  \n",
    "    #~ print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    #~ print(\"WorkerTest._test_AA_match_novor()\")\n",
    "    \n",
    "    peptide_mass = self._compute_peptide_mass(peptide)\n",
    "    prefix_mass = config.mass_AA['_GO'] + np.cumsum([config.mass_AA[aa] for aa in peptide[:-1]])\n",
    "    suffix_mass = peptide_mass - prefix_mass\n",
    "    # mass_loss = np.array([0])\n",
    "    mass_loss = np.array([0, config.mass_H2O, config.mass_NH3])\n",
    "    b_neutral = prefix_mass.reshape((prefix_mass.size, 1)) - mass_loss.reshape((1, -1)) # (m, 3)\n",
    "    y_neutral = suffix_mass.reshape((suffix_mass.size, 1)) - mass_loss.reshape((1, -1))\n",
    "    by_neutral = np.concatenate([b_neutral, y_neutral], axis=1) # (m, 6)\n",
    "    by_charge1 = by_neutral + config.mass_H\n",
    "    by_charge2 = (by_neutral + 2 * config.mass_H) / 2\n",
    "    by_ions = [by_charge1, by_charge2]\n",
    "#     isotopes = [-1, 1]\n",
    "#     by_charge1_iso = [by_charge1 + x * config.mass_H  for x in isotopes]\n",
    "#     by_charge2_iso = [by_charge2 + x * config.mass_H / 2  for x in isotopes]\n",
    "#     by_ions = [by_charge1, by_charge2] + by_charge1_iso + by_charge2_iso\n",
    "    by_ions = np.concatenate(by_ions, axis=1) # (m, i)\n",
    "    \n",
    "    return by_ions\n",
    "\n",
    "\n",
    "  def _match_ion(self, target, predicted, spectrum):\n",
    "    \"\"\"TODO(nh2tran): docstring.\"\"\"\n",
    "  \n",
    "    #~ print(\"\".join([\"=\"] * 80)) # section-separating line\n",
    "    #~ print(\"WorkerTest._test_AA_match_novor()\")\n",
    "    \n",
    "    target_by = self._peptide_to_ions(target).reshape(1, -1)\n",
    "    predicted_by = self._peptide_to_ions(predicted).reshape(1, -1)\n",
    "    mz_nby1 = np.array([x[0] for x in spectrum]).reshape(-1, 1)\n",
    "    target_ion = np.any(np.abs(mz_nby1 - target_by) <= 0.02, axis=1)\n",
    "    predicted_ion = np.any(np.abs(mz_nby1 - predicted_by) <= 0.02, axis=1)\n",
    "    matched_ion = target_ion * predicted_ion\n",
    "    mz_nby1 = mz_nby1.flatten()\n",
    "    unmatched_ion_list = ';'.join([\"{0:.5f}\".format(x) for x in mz_nby1[np.flatnonzero(target_ion * (1-predicted_ion))]])\n",
    "    target_ion = target_ion.sum()\n",
    "    predicted_ion = predicted_ion.sum()\n",
    "    matched_ion = matched_ion.sum()\n",
    "\n",
    "    return target_ion, predicted_ion, matched_ion, unmatched_ion_list\n",
    "\n",
    "\n",
    "# test_accuracy\n",
    "folder = '/data/nh2tran/DeepNovo/DeepDB/aa.pointnovo/Lei/fdr/eval_ABRF/MCP/'\n",
    "target_file = folder + 'pd_merged.csv.db.psms.csv'\n",
    "spectrum_file = folder + '2017-12-4_ABRF_200_DDA1.mgf'\n",
    "for x in range(10, 10+1):\n",
    "    col_score = \"ALC (%)\"\n",
    "    col_aa_score = \"local confidence (%)\"\n",
    "    predicted_file = folder + 'PEAKS/Sample {0:s}.denovo.csv'.format(str(x))\n",
    "    worker_test = WorkerTest(target_file, predicted_file, spectrum_file)\n",
    "    worker_test.test_accuracy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0defd",
   "metadata": {
    "code_folding": [
     0,
     26,
     31,
     107,
     111,
     116,
     124,
     131,
     137
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate decoy mgf\n",
    "print(stop)\n",
    "\n",
    "folder = '/data/nh2tran/DeepNovo/DeepDB/aa.pointnovo/Lei/fdr/eval_ABRF/MCP/'\n",
    "input_mgf_list = [\n",
    "    '2017-12-4_ABRF_200_DDA1.mgf',\n",
    "]\n",
    "input_mgf_list = [folder + x for x in input_mgf_list]\n",
    "\n",
    "peak_sampling = 'random' # random, intensity, intensity_mass, permutation, 500Da, distance\n",
    "print(\"peak_sampling =\", peak_sampling)\n",
    "for sampling_rate in [x / 10. for x in range(5, 5+1)]:\n",
    "    print(\"sampling_rate =\", sampling_rate)\n",
    "    for input_mgf in input_mgf_list:\n",
    "\n",
    "        if peak_sampling == 'permutation':\n",
    "            output_mgf = input_mgf + '.permutation.mgf'\n",
    "        elif peak_sampling == '500Da':\n",
    "            output_mgf = input_mgf + '.500Da.mgf'\n",
    "        else:\n",
    "            output_mgf = input_mgf + '.decoy_{0:.2f}.mgf'.format(sampling_rate)\n",
    "\n",
    "        # collect peak_distr and removed_peaks_distr for noise sampling\n",
    "        if peak_sampling == 'random' or peak_sampling == 'permutation' or peak_sampling == 'distance':\n",
    "            peaks_distr = [[float(x) for x in re.split(' |\\r|\\n', line)[:2]] for line in open(input_mgf, 'r') if line[0].isdigit()]\n",
    "            print(\"len(peaks_distr) =\", len(peaks_distr))\n",
    "        elif peak_sampling == '500Da':\n",
    "            peaks_distr = [[float(x) for x in re.split(' |\\r|\\n', line)[:2]] for line in open(input_mgf, 'r') if line[0].isdigit()]\n",
    "            print(\"len(peaks_distr) =\", len(peaks_distr))\n",
    "            removed_peaks_distr = [[x, y] for x, y in peaks_distr if x < 500]\n",
    "            print(\"len(removed_peaks_distr) =\", len(removed_peaks_distr))\n",
    "        elif peak_sampling == 'intensity' or peak_sampling == 'intensity_mass':\n",
    "            peaks_distr = []\n",
    "            removed_peaks_distr = []\n",
    "            with open(input_mgf, 'r') as f_in:\n",
    "                while True:\n",
    "                    line = f_in.readline()\n",
    "                    if not line: # end of file\n",
    "                        break\n",
    "                    if line == '\\n': # empty line\n",
    "                        continue\n",
    "                    peak_list = []\n",
    "                    while not \"END IONS\" in line:\n",
    "                        # parse header lines\n",
    "                        if 'BEGIN IONS' in line or '=' in line:\n",
    "                            # f_out.write(line)\n",
    "                            line = f_in.readline()\n",
    "                            if 'PEPMASS' in line:\n",
    "                                mz = float(re.split('=| |\\r|\\n', line)[1])\n",
    "                            if 'CHARGE' in line:\n",
    "                                z = float(re.split('=|\\+|\\r|\\n', line)[1])\n",
    "                                peptide_mass = mz * z - z * 1.0078\n",
    "                            continue\n",
    "                        # parse ions\n",
    "                        mz, intensity = re.split(' |\\r|\\n', line)[:2]\n",
    "                        peak_list.append([float(mz), float(intensity)])\n",
    "                        line = f_in.readline()\n",
    "\n",
    "                    # peak removal and noise sampling by intensity\n",
    "                    num_peaks = len(peak_list)\n",
    "                    if peak_sampling == 'intensity':\n",
    "                        num_sampling = int(num_peaks * sampling_rate)\n",
    "                    elif peak_sampling == 'intensity_mass':\n",
    "                        est_len = int(peptide_mass / 122.8652943)\n",
    "                        num_noise = int(min((est_len - 1) * 2, num_peaks) * (1-sampling_rate))\n",
    "                        num_sampling = num_peaks - num_noise\n",
    "                    peak_list_sorted = sorted(peak_list, key=lambda x: x[1])\n",
    "                    # sampling_peaks = peak_list_sorted[:num_sampling]\n",
    "                    removed_peaks = peak_list_sorted[num_sampling:]\n",
    "                    removed_peaks_distr += removed_peaks\n",
    "                    peaks_distr += peak_list\n",
    "            print(\"len(peaks_distr) =\", len(peaks_distr))\n",
    "            print(\"len(removed_peaks_distr) =\", len(removed_peaks_distr))\n",
    "\n",
    "        sampling_peaks_distr, noise_peaks_distr, decoy_peaks_distr = [], [], []\n",
    "        with open(input_mgf, 'r') as f_in:\n",
    "            with open(output_mgf, 'w') as f_out:\n",
    "                while True:\n",
    "                    line = f_in.readline()\n",
    "                    if not line: # end of file\n",
    "                        break\n",
    "                    if line == '\\n': # empty line\n",
    "                        continue\n",
    "                    peak_list = []\n",
    "                    while not \"END IONS\" in line:\n",
    "                        # parse header lines\n",
    "                        if 'BEGIN IONS' in line or '=' in line:\n",
    "                            f_out.write(line)\n",
    "                            line = f_in.readline()\n",
    "                            if 'PEPMASS' in line:\n",
    "                                mz = float(re.split('=| |\\r|\\n', line)[1])\n",
    "                            if 'CHARGE' in line:\n",
    "                                z = float(re.split('=|\\+|\\r|\\n', line)[1])\n",
    "                                peptide_mass = mz * z - z * 1.0078\n",
    "                            if 'SCANS=' in line:\n",
    "                                scan = re.split('=|\\r|\\n', line)[1]\n",
    "                            continue\n",
    "                        # parse ions\n",
    "                        mz, intensity = re.split(' |\\r|\\n', line)[:2]\n",
    "                        peak_list.append([float(mz), float(intensity)])\n",
    "                        line = f_in.readline()\n",
    "                    \n",
    "                    num_peaks = len(peak_list)\n",
    "                    num_sampling = int(num_peaks * sampling_rate)\n",
    "                    num_noise = num_peaks - num_sampling\n",
    "                    random.seed(99); np.random.seed(99)\n",
    "                    # random peak sampling\n",
    "                    if peak_sampling == 'random':\n",
    "                        sampling_peaks = random.sample(peak_list, num_sampling)\n",
    "                        noise_peaks = random.sample(peaks_distr, num_noise)\n",
    "                    # peak removal and noise sampling by intensity\n",
    "                    elif peak_sampling == 'intensity':\n",
    "                        peak_list_sorted = sorted(peak_list, key=lambda x: x[1])\n",
    "                        sampling_peaks = peak_list_sorted[:num_sampling]\n",
    "                        noise_peaks = random.sample(removed_peaks_distr, num_noise)\n",
    "                    # peak removal and noise sampling by intensity and peptide mass\n",
    "                    elif peak_sampling == 'intensity_mass':\n",
    "                        est_len = int(peptide_mass / 122.8652943)\n",
    "                        num_noise = int(min((est_len - 1) * 2, num_peaks) * (1-sampling_rate))\n",
    "                        num_sampling = num_peaks - num_noise\n",
    "                        peak_list_sorted = sorted(peak_list, key=lambda x: x[1])\n",
    "                        sampling_peaks = peak_list_sorted[:num_sampling]\n",
    "                        noise_peaks = random.sample(removed_peaks_distr, num_noise)\n",
    "                    # peak permutation\n",
    "                    elif peak_sampling == 'permutation':\n",
    "                        mz_list = [x[0] for x in peak_list]\n",
    "                        intensity_list = [x[1] for x in peak_list]\n",
    "                        sampling_peaks = []\n",
    "                        random.shuffle(intensity_list)\n",
    "                        noise_peaks = list(zip(mz_list, intensity_list))\n",
    "                    # remove peaks under 500 Da and replace by noise peaks\n",
    "                    elif peak_sampling == '500Da':\n",
    "                        sampling_peaks = [[x, y] for x, y in peak_list if x >= 500]\n",
    "                        num_sampling = len(sampling_peaks)\n",
    "                        num_noise = num_peaks - num_sampling\n",
    "                        noise_peaks = random.sample(removed_peaks_distr, num_noise)\n",
    "                    # peak removal by distance\n",
    "                    elif peak_sampling == 'distance':\n",
    "                        mz_array = np.array([peak[0] for peak in peak_list])\n",
    "                        pair_distance = np.absolute(np.reshape(mz_array, (num_peaks, 1)) - np.reshape(mz_array, (1, num_peaks)))\n",
    "                        aa_masses = config.mass_ID_np[3:].reshape(1, -1)\n",
    "                        pair_aa_match = np.absolute(np.expand_dims(pair_distance, axis=2) - aa_masses) # (n, n, aa)\n",
    "                        pair_aa_match = np.any(pair_aa_match <= 0.02, axis=2)\n",
    "                        match_peak_indices = list(np.flatnonzero(np.any(pair_aa_match, axis=1)))\n",
    "                        num_noise = int(len(match_peak_indices) * (1-sampling_rate))\n",
    "                        removed_indices = random.sample(match_peak_indices, num_noise)\n",
    "                        sampling_peaks = [peak for index, peak in enumerate(peak_list) if index not in removed_indices]\n",
    "                        noise_peaks = random.sample(peaks_distr, num_noise)\n",
    "                    sampling_peaks_distr += sampling_peaks\n",
    "                    noise_peaks_distr += noise_peaks\n",
    "                    decoy_peaks_distr += sampling_peaks + noise_peaks\n",
    "\n",
    "                    # write ion lines\n",
    "                    sorted_peaks = sorted(sampling_peaks + noise_peaks, key=lambda x: x[0])\n",
    "                    for x, y in sorted_peaks: f_out.write(\"{0:.5f} {1:.5f}\\n\".format(x, y))\n",
    "                    f_out.write(line) # END IONS line\n",
    "                    f_out.write(f_in.readline()) # empty line between spectra\n",
    "        print(\"len(sampling_peaks_distr) =\", len(sampling_peaks_distr))\n",
    "        print(\"len(noise_peaks_distr) =\", len(noise_peaks_distr))\n",
    "        print(\"len(decoy_peaks_distr) =\", len(decoy_peaks_distr))\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b232592",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d20c3a1c",
   "metadata": {
    "code_folding": [
     0,
     3,
     11
    ]
   },
   "outputs": [],
   "source": [
    "# Decoy FDR calculation\n",
    "print(stop)\n",
    "\n",
    "def read_denovo(denovo_csv, selected_features=None):\n",
    "    denovo_psm = pd.read_csv(denovo_csv, keep_default_na=False)\n",
    "    denovo_psm['feature_id'] = denovo_psm.apply(lambda row: row['Source File'].split('.mgf')[0] + '.mgf' + '||' + str(row['Scan']), axis=1)\n",
    "    if selected_features:\n",
    "        denovo_psm['selected_features'] = denovo_psm.apply(lambda row: row['feature_id'] in selected_features, axis=1)\n",
    "        denovo_psm = denovo_psm[denovo_psm['selected_features']]\n",
    "    return denovo_psm\n",
    "\n",
    "def calculate_FDR(target_csv, decoy_csv, engine_score, fdr_list, selected_features=None):\n",
    "    \n",
    "    print(\"target_csv =\", target_csv)\n",
    "    print(\"decoy_csv =\", decoy_csv)\n",
    "    target_psm = read_denovo(target_csv, selected_features)\n",
    "    decoy_psm = read_denovo(decoy_csv, selected_features)\n",
    "    print(\"len(target_psm) =\", len(target_psm)); print(\"len(decoy_psm) =\", len(decoy_psm))\n",
    "    dfs = pd.concat([target_psm, decoy_psm], keys=['target', 'decoy']).reset_index().rename(columns={'level_0': 'spectrum'})\n",
    "    dfs['is_target'] = dfs.apply(lambda row: row['spectrum']=='target', axis=1)\n",
    "\n",
    "    # target-decoy competition\n",
    "    dfs.sort_values(by=[engine_score, 'is_target'], ascending=[False, False], inplace=True)\n",
    "    # 1-1 competition on each scan id\n",
    "#     dfs_fdr = dfs.drop_duplicates(subset=['feature_id'])\n",
    "    # competition on whole dataset\n",
    "    dfs_fdr = dfs\n",
    "    dfs_fdr['feature_id'] = dfs_fdr.apply(lambda x: x['feature_id'] if x['is_target'] else x['feature_id']+'||decoy', axis=1)\n",
    "    print(\"len(dfs) =\", len(dfs))\n",
    "    print(\"len(dfs_fdr) =\", len(dfs_fdr))\n",
    "    print(\"sum(dfs_fdr['is_target']) =\", sum(dfs_fdr['is_target']))\n",
    "    \n",
    "    # fdr estimation\n",
    "    cumsum = range(1, len(dfs_fdr) + 1)\n",
    "    cumsum_target = np.cumsum(np.array(dfs_fdr['is_target'].astype(int)))\n",
    "    cumsum_decoy = cumsum - cumsum_target\n",
    "    estimated_fdr = cumsum_decoy / cumsum_target\n",
    "    dfs_fdr['estimated_fdr'] = estimated_fdr\n",
    "\n",
    "    score_list = []\n",
    "    count_list = []\n",
    "    for fdr in fdr_list:\n",
    "        fdr_index = np.flatnonzero(estimated_fdr <= fdr)\n",
    "        fdr_index = fdr_index[-1] if len(fdr_index) > 0 else 0\n",
    "        score_list.append(dfs_fdr.iloc[fdr_index][engine_score])\n",
    "        count_list.append(fdr_index + 1)\n",
    "\n",
    "    return dfs, dfs_fdr, score_list, count_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e49e689e",
   "metadata": {
    "code_folding": [
     0,
     3
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Decoy FDR validation\n",
    "print(stop)\n",
    "\n",
    "def validate_FDR(target_csv, decoy_csv, engine_score, db_csv, spectrum_file, p_decoy, T_pct):\n",
    "\n",
    "    db_psm = pd.read_csv(db_csv, keep_default_na=False)\n",
    "    db_psm['feature_id'] = db_psm.apply(lambda row: row['Source File'].split('.mgf')[0] + '.mgf' + '||' + str(row['Scan']), axis=1)\n",
    "    selected_features = None # set(db_psm['feature_id'])\n",
    "\n",
    "    dfs, dfs_fdr, score_list, count_list = calculate_FDR(target_csv, decoy_csv, engine_score, p_decoy, selected_features)\n",
    "\n",
    "    target_decoy_csv = target_csv + '-' + decoy_csv.split('/')[-1]\n",
    "    dfs_fdr.to_csv(target_decoy_csv, index=False)\n",
    "    accuracy_file = target_decoy_csv + '.accuracy'\n",
    "    if not os.path.isfile(accuracy_file):\n",
    "        col_score = engine_score\n",
    "        worker_test = WorkerTest(db_csv, target_decoy_csv, spectrum_file)\n",
    "        worker_test.test_accuracy()\n",
    "\n",
    "    denovo_df = dfs_fdr\n",
    "    denovo_df = denovo_df.set_index('feature_id')\n",
    "    accuracy_df = pd.read_csv(accuracy_file, delimiter ='\\t', index_col='feature_id')\n",
    "    denovo_df['db_peptide'] = accuracy_df.apply(\n",
    "        lambda x: x['target_sequence'].replace('C(Carbamidomethylation)', 'C(+57.02)').replace('M(Oxidation)', 'M(+15.99)').replace(',', ''), \n",
    "        axis=1)\n",
    "    denovo_df['recall_AA'] = accuracy_df['recall_AA']\n",
    "    denovo_df['predicted_len'] = accuracy_df['predicted_len']\n",
    "    denovo_df['recall_peptide'] = accuracy_df.apply(lambda x: x['recall_AA'] == x['predicted_len'], axis=1)\n",
    "    denovo_df['target_ion'] = accuracy_df['target_ion']\n",
    "    denovo_df['matched_ion'] = accuracy_df['matched_ion']\n",
    "    denovo_df['recall_peptide_I'] = accuracy_df.apply(lambda x: x['matched_ion'] == x['target_ion'], axis=1)\n",
    "    denovo_df['recall_peptide_T'] = accuracy_df.apply(lambda x: x['matched_ion'] >= x['target_ion']*T_pct, axis=1)\n",
    "    print(\"len(denovo_df) =\", len(denovo_df))\n",
    "    print(\"  with recall_AA =\", len(denovo_df[~denovo_df['recall_AA'].isna()]))\n",
    "    print(\"    is_target =\", len(denovo_df[~denovo_df['recall_AA'].isna()][denovo_df['is_target']]))\n",
    "\n",
    "    # calculate true FDR on annotated target spectra\n",
    "    df = denovo_df\n",
    "    df = df[~df['recall_AA'].isna()]\n",
    "    df = df[df['is_target']]\n",
    "    cumsum = range(1, len(df) + 1)\n",
    "    cumsum_correct = np.cumsum(np.array(df['recall_peptide'].astype(int)))\n",
    "    cumsum_false = cumsum - cumsum_correct\n",
    "    true_fdr = cumsum_false / cumsum\n",
    "    cumsum_correct = np.cumsum(np.array(df['recall_peptide_I'].astype(int)))\n",
    "    cumsum_false = cumsum - cumsum_correct\n",
    "    true_fdr_I = cumsum_false / cumsum\n",
    "    cumsum_correct = np.cumsum(np.array(df['recall_peptide_T'].astype(int)))\n",
    "    cumsum_false = cumsum - cumsum_correct\n",
    "    true_fdr_T = cumsum_false / cumsum\n",
    "    # only report entries with decreasing fdr from the bottom to avoid bumps\n",
    "    min_est, min_true = 1, 1\n",
    "    reported = []\n",
    "    for x, y, z, v, w in list(zip(df['estimated_fdr'], cumsum, true_fdr, true_fdr_I, true_fdr_T))[::-1]:\n",
    "        if x <= min_est and w <= min_true:\n",
    "            min_est = x\n",
    "            min_true = w\n",
    "            reported.append((x, y, z, v, w))\n",
    "    estimated_fdr, cumsum, true_fdr, true_fdr_I, true_fdr_T = zip(*reported)\n",
    "    \n",
    "    # calculate estimated FDR and #PSMs on all target spectra\n",
    "    df = denovo_df\n",
    "    df = df[df['is_target']]\n",
    "    cumsum_full = range(1, len(df) + 1)\n",
    "    # only report entries with decreasing fdr from the bottom to avoid bumps\n",
    "    min_est = 1\n",
    "    reported = []\n",
    "    for x, y in list(zip(df['estimated_fdr'], cumsum_full))[::-1]:\n",
    "        if x <= min_est:\n",
    "            min_est = x\n",
    "            reported.append((x, y))\n",
    "    estimated_fdr_full, cumsum_full = zip(*reported)\n",
    "    \n",
    "    return {\n",
    "        'denovo_df': denovo_df, \n",
    "        'df': df, \n",
    "        'estimated_fdr': estimated_fdr,\n",
    "        'cumsum': cumsum,\n",
    "        'true_fdr': true_fdr,\n",
    "        'true_fdr_I': true_fdr_I,\n",
    "        'true_fdr_T': true_fdr_T,\n",
    "        'estimated_fdr_full': estimated_fdr_full,\n",
    "        'cumsum_full': cumsum_full,\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47bc54",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Decoy FDR validation VS X% removed peaks\n",
    "print(stop)\n",
    "\n",
    "folder = '/data/nh2tran/DeepNovo/DeepDB/aa.pointnovo/Lei/fdr/eval_ABRF/MCP/'\n",
    "db_csv = folder + 'pd_merged.csv.db.psms.csv'\n",
    "spectrum_file = folder + '2017-12-4_ABRF_200_DDA1.mgf'\n",
    "\n",
    "p_decoy = [x/1000. for x in range(0, 50, 1)]\n",
    "T_pct = 0.90\n",
    "\n",
    "samples = range(3, 7+1)\n",
    "target_csv = folder + 'PEAKS/Sample 10.denovo.csv'\n",
    "decoy_csv_list = [folder + 'PEAKS/Sample {0:s}.denovo.csv'.format(str(x)) for x in samples]\n",
    "engine_score = 'ALC (%)'\n",
    "col_score = engine_score\n",
    "col_aa_score = 'local confidence (%)'\n",
    "results_list = [validate_FDR(target_csv, decoy_csv, engine_score, db_csv, spectrum_file, p_decoy, T_pct) \n",
    "                for decoy_csv in decoy_csv_list]\n",
    "\n",
    "fig, ax = pyplot.subplots(1, 2, figsize=(9,4))\n",
    "labels = ['decoy {0:s}0%'.format(str(10-x)) for x in samples]\n",
    "colors9 = ['b', 'c', 'g', 'k', 'm', 'r', 'y', 'orange', 'pink']\n",
    "colors = [colors9[x-1] for x in samples]\n",
    "ylim = 0\n",
    "for results, c, l in zip(results_list, colors, labels):\n",
    "    ax[0].plot(results['estimated_fdr'], results['true_fdr_T'], color=c, label=l)\n",
    "    ax[1].plot(results['cumsum'], results['estimated_fdr'], color=c, label=l)\n",
    "ax[0].plot([0, 0.05], [0, 0.05], color='black', linestyle='--', label='True FDR')\n",
    "ax[0].set_xlim(0, 0.05)\n",
    "# ax[0].set_ylim(0, 0.10)\n",
    "ax[0].set_xlabel('Estimated FDR'); ax[0].set_ylabel('True FDR')\n",
    "ax[0].legend()\n",
    "ax[1].plot(results_list[2]['cumsum'], results_list[2]['true_fdr_T'], color='black', label='True FDR', linestyle='--')\n",
    "ax[1].set_ylim(0, 0.05)\n",
    "ax[1].set_xlabel('Number of PSMs'); ax[1].set_ylabel('FDR')\n",
    "# ax[1].legend()\n",
    "fig.tight_layout()\n",
    "# fig.savefig('fig/fig.decoy_fdr_valid_X_random_abrf_peaks.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a612d58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_tf2",
   "language": "python",
   "name": "python3_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
